{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "# This is the simple circuit we will run if the CHSH test passes\n",
    "# It creates a Bell state and measures both qubits\n",
    "pli_plus = QuantumCircuit(2, 2)\n",
    "pli_plus.h(0)\n",
    "pli_plus.cx(0, 1)\n",
    "pli_plus.measure([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637adc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 100\n",
    "delay_between_reps = 10  # Every 10 seconds; change this if you are using a real backend \n",
    "introspection_shots = 1024\n",
    "main_circuit_shots = 1024\n",
    "entanglement_threshold = 1.9 # CHSH score above which we consider the backend to be entangling - 2.8 in Aer simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from qonscious.adapters import AerSamplerAdapter, BackendAdapter, IBMSamplerAdapter\n",
    "\n",
    "# Uncomment this line to use the Aer simulator instead of a real\n",
    "backend_adapter = AerSamplerAdapter()\n",
    "\n",
    "# Uncomment these lines to use a real backend instead of the Aer simulator\n",
    "# ibm_token = os.getenv(\"IBM_QUANTUM_TOKEN\")\n",
    "# backend_adapter = IBMSamplerAdapter.least_busy_backend(ibm_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import TYPE_CHECKING, cast\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from qonscious.core import ScorableFigureOfMeritResult\n",
    "\n",
    "from qonscious.core import FigureOfMeritResult\n",
    "\n",
    "\n",
    "def on_pass(backend_adapter : BackendAdapter, figure_of_merit_results: list[FigureOfMeritResult]):\n",
    "    chsh_score = cast(\"ScorableFigureOfMeritResult\",figure_of_merit_results[0])[\"score\"]\n",
    "    print(f\"Entanglement passed with score: {chsh_score}\")\n",
    "    print(\"Running main circuit (creating a Bell state and measuring it)...\")\n",
    "    run_result = backend_adapter.run(pli_plus, shots=main_circuit_shots)\n",
    "    print(\"Main circuit run complete.\")\n",
    "    return run_result\n",
    "\n",
    "def on_fail(backend_adapter : BackendAdapter, figure_of_merit_results: list[FigureOfMeritResult]):\n",
    "    chsh_score = cast(\"ScorableFigureOfMeritResult\",figure_of_merit_results[0])[\"score\"]\n",
    "    print(f\"Skipping main circuit - entanglement score was {chsh_score}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonscious.core import MeritComplianceCheck\n",
    "from qonscious.foms import PackedCHSHTest\n",
    "\n",
    "\n",
    "def chsh_score_over(threshold: float):\n",
    "    return lambda r: cast(\"ScorableFigureOfMeritResult\", r)[\"score\"] > threshold\n",
    "\n",
    "check_chsh_is_ok = check = MeritComplianceCheck(\n",
    "    figure_of_merit=PackedCHSHTest(),\n",
    "    decision_function=chsh_score_over(2.0), # In the Aer simulator we can get up to 2.828\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from qonscious.core.executor import run_conditionally\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(reps):\n",
    "    print(f\"{i+1}. Running figure of merit checks + conditional execution\")\n",
    "    qonscious_result = run_conditionally(\n",
    "    backend_adapter=backend_adapter,\n",
    "    checks= [check_chsh_is_ok],\n",
    "    on_pass=on_pass,\n",
    "    on_fail=on_fail,\n",
    "    shots=2048\n",
    "    )\n",
    "    results.append(qonscious_result)\n",
    "    if i < reps - 1:\n",
    "        print(f\"Sleeping for {delay_between_reps} seconds\")\n",
    "        time.sleep(delay_between_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chsh_scores = [result[\"figures_of_merit_results\"][0][\"score\"] for result in results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(chsh_scores)+1), chsh_scores, marker='o', linestyle='-', color='b', label=\"Observed\")\n",
    "\n",
    "# Add Tsirelsonâ€™s bound\n",
    "tsirelson_bound = 2.828\n",
    "plt.axhline(tsirelson_bound, color='r', linestyle='--', label=\"Tsirelson's bound\")\n",
    "\n",
    "plt.title(\"CHSH Score Over Repetitions\")\n",
    "plt.xlabel(\"Run Number\")\n",
    "plt.ylabel(\"CHSH Score\")\n",
    "# plt.ylim(bottom=0)   # start y-axis at 0\n",
    "# plt.ylim(top=3)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parse_utc(s):\n",
    "    return datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "time_gaps = [\n",
    "    (parse_utc(result[\"figures_of_merit_results\"][0][\"experiment_result\"][\"timestamps\"][\"finished\"]) -\n",
    "     parse_utc(result[\"experiment_result\"][\"timestamps\"][\"running\"])).total_seconds()\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(time_gaps)), time_gaps, marker='o', color='orange')\n",
    "plt.title(\"Total time gaps from completion of FoM check to start of main circuit\")\n",
    "plt.xlabel(\"Run Index\")\n",
    "plt.ylabel(\"Time gaps (s)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2439b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_plus_fidelity(counts):\n",
    "    shots = sum(counts.values())\n",
    "    prob_00 = counts.get(\"00\", 0) / shots\n",
    "    prob_11 = counts.get(\"11\", 0) / shots\n",
    "    # For ideal \\Phi^+, the off-diagonal terms contribute too, but from counts only, we approximate fidelity\n",
    "    return prob_00 + prob_11  # coarse classical fidelity estimate\n",
    "\n",
    "fidelities = [phi_plus_fidelity(result[\"experiment_result\"][\"counts\"]) for result in results]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(fidelities)), fidelities, marker='o', color='green')\n",
    "plt.title(\"Approximate Fidelity to Phi+ State\")\n",
    "plt.xlabel(\"Run Index\")\n",
    "plt.ylabel(\"Fidelity Estimate\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix this cell to the new structure of results\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Save experiment data in a tabular summary\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "csv_filename = f\"experiment_summary_{timestamp}.csv\"\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    'introspection_created': r.get('introspection', {}).get('timestamps', {}).get('created'),\n",
    "    'introspection_running': r.get('introspection', {}).get('timestamps', {}).get('running'),\n",
    "    'introspection_finished': r.get('introspection', {}).get('timestamps', {}).get('finished'),\n",
    "    'execution_created': r.get('execution', {}).get('timestamps', {}).get('created'),\n",
    "    'execution_running': r.get('execution', {}).get('timestamps', {}).get('running'),\n",
    "    'execution_finished': r.get('execution', {}).get('timestamps', {}).get('finished'),    \n",
    "    'entanglement_score': r.get('entanglement_score'),\n",
    "    'execution_counts': json.dumps(r.get('execution', {}).get('counts')) if r.get('execution') else None\n",
    "} for r in results])\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e69aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
